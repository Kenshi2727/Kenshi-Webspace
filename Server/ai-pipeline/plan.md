# ai-pipeline-plan.md

AI Development Strategy for Kenshi Webspace

---

## Overview

This document summarizes two possible architectural paths for integrating AI functionality into Kenshi Webspace:

1. **Monolithic Architecture** (Node.js + Python inside the same project under `/src`)
2. **Microservice Architecture** (Separate Python AI pipeline alongside Node.js)

This file explains:

* When each approach makes sense
* Pros and cons
* Technical implications
* Deployment concerns
* Scaling and maintenance trade-offs

This is meant to guide future development decisions when the AI pipeline is implemented.

---

# 1ï¸âƒ£ Option A â€“ Monolithic Architecture

**(Python inside `src/ai/` and controlled directly by Node)**

## ğŸ“Œ Structure

```
Server/
â””â”€ src/
   â”œâ”€ controllers/
   â”œâ”€ routes/
   â”œâ”€ middlewares/
   â”œâ”€ utils/
   â””â”€ ai/
      â”œâ”€ pipeline.py
      â”œâ”€ langchain_processor.py
      â””â”€ embeddings.py
```

## ğŸ”§ How it works

* Node.js invokes Python scripts using:

  * `child_process`
  * `python-shell`
  * local bindings
* Everything is one application and one deployment.

## ğŸ§  When this approach is good

* Small to medium system
* Local machine or single-server deployment
* No Kafka or external workers
* Node and Python tightly coupled
* Fast development without infrastructure complexity
* AI is helper logic, not a standalone service

## âš™ Pros

* Easier to develop
* Single deployment
* No networking between Node & Python
* Easier debugging
* No need for container orchestration

## âŒ Cons

* If Python crashes, whole backend may crash
* Node and Python can't scale independently
* Harder to Dockerize or deploy in distributed environments
* Hard to add:

  * Kafka consumers
  * Background AI workers
  * Multiple GPU/CPU processing units
* Logs and resource usage become mixed

## ğŸ§© Best for

Early stages of Kenshi Webspace where:

* AI usage is small
* Low user traffic
* No heavy compute
* Want fast results

---

# 2ï¸âƒ£ Option B â€“ Microservice Architecture

**(Python in a separate directory such as `/ai-pipeline` running its own process)**

## ğŸ“Œ Structure

```
Server/
â”œâ”€ src/              â†’ Node.js main backend
â””â”€ ai-pipeline/      â†’ Python AI microservice
   â”œâ”€ main.py
   â”œâ”€ workers/
   â”œâ”€ llm/
   â””â”€ utils/
```

## ğŸ”§ How it works

* Python runs independently using:

  * FastAPI/Flask
  * Kafka consumers
  * Workers
* Node communicates through:

  * REST API
  * gRPC
  * Kafka
  * Redis queue
* Each service has its own:

  * Dependencies
  * Deployment
  * Resources
  * Runtime

## ğŸ§  When this approach is best

* Large user base
* Heavy or frequent AI processing
* Python needs:

  * LangChain pipelines
  * Embeddings
  * Vector stores
  * Model hosting
* AI might run on:

  * GPU servers
  * Containers
  * Auto-scaling clusters

## âš™ Pros

* Node and Python scale independently
* If Python crashes, Node continues normally
* Better resource monitoring
* Fits distributed architectures
* Supports:

  * Kafka
  * Workers
  * Long-running tasks
* Clean DevOps separation

## âŒ Cons

* More complex deployment
* Needs networking between services
* Requires Docker or service orchestration
* Slightly slower development pace early on

## ğŸš€ Best for

Future production version of Kenshi Webspace where:

* AI becomes a core feature
* Needs separate scaling
* Needs uptime and resilience
* Heavy traffic is expected

---

# ğŸ”„ Decision Rule Summary

| Question                                         | If YES â†’             | If NO â†’            |
| ------------------------------------------------ | -------------------- | ------------------ |
| Will Python be its own API or Kafka worker?      | Use **Microservice** | Monolithic is fine |
| Will AI need separate scaling?                   | Microservice         | Monolithic         |
| Small/medium system with single deployment?      | Monolithic           | Microservice       |
| Will Node and Python run in separate containers? | Microservice         | Monolithic         |
| Is the goal fast early development?              | Monolithic           | â€”                  |
| Is this for long-term production readiness?      | Microservice         | â€”                  |

---

# ğŸ’¡ Recommended Evolution Path

### Phase 1 (Early development)

* Use **Monolithic approach**
* AI under `src/ai/`
* Minimal setup
* No Kafka needed

### Phase 2 (Growth)

* Split into dedicated service:

```
Server/
â”œâ”€ src/          (Node)
â””â”€ ai-pipeline/  (Python)
```

### Phase 3 (Scaling)

* Add:

  * Kafka
  * Docker
  * independent deployment
  * GPU compute if needed




# ai-pipeline-plan.md

AI Development Strategy for Kenshi Webspace

---

## Overview

This document summarizes two possible architectural paths for integrating AI functionality into Kenshi Webspace:

1. **Monolithic Architecture** (Node.js + Python inside the same project under `/src`)
2. **Microservice Architecture** (Separate Python AI pipeline alongside Node.js)

This file explains:

* When each approach makes sense
* Pros and cons
* Technical implications
* Deployment concerns
* Scaling and maintenance trade-offs
* Kafka usage design
* Docker deployment guidance
* Migration plan for small â†’ large scale

Reference document when starting AI development later.

---
<br>
<br>
<br>

---

# Code examples for refrerence and clarity.


# 1ï¸âƒ£ Option A â€“ Monolithic Architecture

**Node + Python under the same project (`/src`)**

## Folder Structure

```
Server/
â””â”€ src/
   â”œâ”€ controllers/
   â”œâ”€ routes/
   â”œâ”€ middlewares/
   â”œâ”€ utils/
   â””â”€ ai/
      â”œâ”€ pipeline.py
      â”œâ”€ langchain_processor.py
      â”œâ”€ embeddings.py
      â””â”€ sentiment.py
```

## How It Works

Node directly executes Python scripts using:

* `child_process`
* `python-shell`
* local execution

Everything runs as **one backend service**.

## System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node.js    â”‚
â”‚ (web server)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Local call
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python scriptsâ”‚
â”‚ (inside /src) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 2ï¸âƒ£ Option B â€“ Microservice Architecture

**Python AI lives separately (`/ai-pipeline`) and runs independently**

## Folder Structure

```
Server/
â”œâ”€ src/              â†’ Node backend
â””â”€ ai-pipeline/      â†’ Python microservice
   â”œâ”€ main.py
   â”œâ”€ workers/
   â”œâ”€ llm/
   â””â”€ utils/
```

## System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP / gRPC / Kafka     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node.js   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚ Python AI Pipeline â”‚
â”‚  Backend   â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚ (FastAPI,LangChain)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

# Kafka Integration (Recommended for Microservice)

## Example Flow

```
[User â†’ Kenshi frontend]
         â”‚
         â–¼
     Node API
         â”‚  produces event
         â–¼
      Kafka topic
         â”‚
         â–¼
Python AI consumer
         â”‚
         â–¼
 vector DB / results
         â”‚
         â–¼
Node fetches and serves result
```

## Event Types

Possible Kafka topics:

* `content.summarize`
* `post.generateEmbeddings`
* `seo.score`
* `recommendation.compute`

## Benefits

âœ” Decoupled architecture
âœ” No request timeout issues
âœ” Can run long AI tasks safely
âœ” Scales horizontally

---

# Docker Deployment Notes

## Monolithic (Single Container)

**`Dockerfile`**

```
FROM node:20
WORKDIR /app
COPY . .
RUN npm install
CMD ["node", "src/index.js"]
```

Run Python inside from Node.

Good for early stage.

---

## Microservice (Two Containers)

### Node container

```
FROM node:20
WORKDIR /app
COPY ./server .
RUN npm install
CMD ["node", "src/index.js"]
```

### Python AI container

```
FROM python:3.12
WORKDIR /ai
COPY ./ai-pipeline .
RUN pip install -r requirements.txt
CMD ["python", "main.py"]
```

### docker-compose

```
services:
  api-server:
    build: ./server
    ports:
      - "3000:3000"
    depends_on:
      - ai-pipeline

  ai-pipeline:
    build: ./ai-pipeline
    ports:
      - "5000:5000"
```

If Kafka is used:

```
  kafka:
  zookeeper:
```

---

# Migration Plan (Recommended Path)

## Phase 1 (Early Development)

* Use Monolithic:

  * Python under `/src/ai/`
* No Kafka
* Node calls Python locally

### Why?

Fastest development.

---

## Phase 2 (Medium Scale)

Move Python to its own folder:

```
/ai-pipeline
```

But still run both on the same machine.

---

## Phase 3 (Production Scale)

* Use Docker or Kubernetes
* Add Kafka
* Deploy Node and Python separately
* Add monitoring:

  * Prometheus
  * Grafana
  * Loki logs

---



